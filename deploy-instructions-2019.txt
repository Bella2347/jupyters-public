# These are instructions for deploying Apache Spark.
# They include a hacks relevant for the SNIC cloud, which we wouldn't use for a production system.
# For the lab, a spark cluster has already been deployed -- this will be an experiment!
# Create a virtual machine (or use an existing one), and connect to the Spark cluster as a client.
# Your virtual machine should be 'ssc.small' flavor.
# Add it to the 'ben' security group for it to work correctly with Spark.
# Add a floating IP, and configure your ~/.ssh/config like this:

# replace 130.238.x.y and ~/.ssh/id_rsa with your floating IP and key path appropriately.
Host 130.238.x.y
  User ubuntu
  IdentityFile ~/.ssh/id_rsa
  LocalForward 8888 localhost:8888
  LocalForward 4040 localhost:4040
  LocalForward 4041 localhost:4041
  LocalForward 4042 localhost:4042
  LocalForward 4043 localhost:4043


#####################

# Hack to fix issue with ubuntu packages
sudo sed -ie 's/nova.clouds.archive.ubuntu.com/se.archive.ubuntu.com/' /etc/apt/sources.list 

## For this example, we'll install Spark worker and master on the same virual machine. Normally we'd put the master on its own machine.

# update apt repo metadata
sudo apt update

# install java
sudo apt-get install -y openjdk-8-jdk


# manually define a hostname for all the hosts on the ldsa project. this will make networking easier with spark:
# if you have added entries to /etc/hosts yourself, you need to remove those.
for i in {1..255}; do echo "192.168.1.$i host-192-168-1-$i-ldsa" | sudo tee -a /etc/hosts; done
for i in {1..255}; do echo "192.168.2.$i host-192-168-2-$i-ldsa" | sudo tee -a /etc/hosts; done

# set the hostname according to the scheme above:
sudo hostname host-$(hostname -I | awk '{$1=$1};1' | sed 's/\./-/'g)-ldsa ; hostname


# ... the next steps depend on what we want this node to be... a master, a worker, or run a Python notebook...
# For the lab, we'll just create a Python notebook, and use the existing Spark cluster.

########################################################################################################################
##### Start Spark Master/Worker -- SKIP THIS FOR THE LAB SESSION: Proceed to 'Install the Python Notebook...') #####

cd ~

# download Spark
# (Get download link for Hadoop 2.7)
wget http://apache.mirrors.spacedump.net/spark/spark-2.4.1/spark-2.4.1-bin-hadoop2.7.tgz

tar -zxvf spark-2.4.1-bin-hadoop2.7.tgz

# Tell Spark that we're using Python 3 -- we need to use the same version of Python everywhere.
echo "export SPARK_HOME=~/spark-2.4.1-bin-hadoop2.7" >> ~/.bashrc
source ~/.bashrc


cd ~/spark-2.4.1-bin-hadoop2.7/


# lets have a look at some of the spark directories..
ls -l

# start the master on the current machine:
~/spark-2.4.1-bin-hadoop2.7/sbin/start-master.sh

# -or-

# start the worker on the current machine (and tell it where the master is listening -- the IP address of our master node from above):
~/spark-2.4.1-bin-hadoop2.7/sbin/start-slave.sh spark://192.168.1.153:7077

netstat -tna

# is it running?
jps


########################################################################################################################
##### Install the Python Notebook -- RESUME HERE FOR THE LAB  #####

# Env variable so the workers know which Python to use...
echo "export PYSPARK_PYTHON=python3" >> ~/.bashrc
source ~/.bashrc

# install git
sudo apt-get install -y git

# install python dependencies, start notebook

# install the python package manager 'pip' -- it is recommended to do this directly 
sudo apt-get install -y python3-pip

# this is a very old version of pip:
python3 -m pip --version

# upgrade it
python3 -m pip install pip

# install jupyter (installing via pip seems to be broken)
sudo apt install -y jupyter-notebook

# install pyspark, jupyter, and some other useful deps
python3 -m pip install pyspark --user
python3 -m pip install pandas --user
python3 -m pip install matplotlib --user

# clone the examples from the lectures, so we have a copy to experiment with
git clone https://github.com/benblamey/jupyters-public.git

# start the notebook!
jupyter notebook

# Follow the instructions you see: 
#
#    Copy/paste this URL into your browser when you connect for the first time,
#    to login with a token:
#        http://localhost:8888/?token=8af4be03b08713c66ed8d093a7d684108c69c86f5b63dd



########################################################################################################################

# You need to share the Spark cluster with the other students:

# 1. Start your application with dynamic allocation enabled, a timeout of no more than 30 seconds, and a cap on CPU cores:
#spark_session = SparkSession\
#        .builder\
#        .master("spark://master:7077") \
#        .appName("blameyben_lecture1_simple_example")\
#        .config("spark.dynamicAllocation.enabled", True)\
#        .config("spark.shuffle.service.enabled", True)\
#        .config("spark.dynamicAllocation.executorIdleTimeout","30s")\
#        .config("spark.executor.cores",4)\
#        .getOrCreate()

# 2. Put your name in the name of your application.
# 3. Kill your application when your have finished with it.
# 4. Don't interfere with any of the virtual machines in the cluster.
# 5. Run one app at a time.
# 6. When the lab is not running, you can use more resources, but keep an eye on other people using the system.

# Now you can run the examples from the lectures in your own notebook.
# Start with https://github.com/benblamey/jupyters-public/blob/master/ldsa-2019/Lecture1_Example1_ArraySquareandSum.ipynb
# You'll need to change the host name for the Spark master, and namenode.

# P.S. Backup your code from the SNIC Cloud!